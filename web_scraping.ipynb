{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web-scraping.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMKmDT8t3W/Y3hzlyvGZjWi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pauloigorms/web-scraping-olx/blob/main/web_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT5XUNQte75S"
      },
      "source": [
        "<p><img alt=\"Ícone importando do portal FlatIcon\" width=\"30\" src=\"https://www.flaticon.com/svg/static/icons/svg/2742/2742225.svg\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1><strong>Web Scraping | OLX E-commerce</strong></h1>\n",
        "\n",
        "&emsp; Projeto independente sem fins lucrativos. O intuito escora-se em acrescentar e fixar conhecimentos em <b>web scraping</b> adquiridos durante o curso promovido pelo Instituto SIDIA | Manaus - AM.\n",
        "\n",
        "<br />\n",
        "\n",
        "<i>Por Paulo Igor Moraes</i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRM7MePkjf2z"
      },
      "source": [
        "!pip install dnspython\r\n",
        "!pip3 install pymongo[srv]\r\n",
        "\r\n",
        "# run and restart notebook colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuGgYdkYekx4"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "import pandas as pd\n",
        "import json\n",
        "from pymongo import MongoClient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at2-9F2EK594"
      },
      "source": [
        "client = MongoClient(\"mongodb+srv://ueacid:nd3S6HJmp3T7aGeR@catalog.j1eua.mongodb.net/websolx?retryWrites=true&w=majority\")\n",
        "db = client.get_database('websolx')\n",
        "property = db.property"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoSu5y38lFlq"
      },
      "source": [
        "def getDataOLX(pages = 2):\n",
        "\n",
        "  results = []\n",
        "  url_based = 'https://am.olx.com.br/regiao-de-manaus/manaus/imoveis?o='\n",
        "  params = {\n",
        "    'authority': 'am.olx.com.br',\n",
        "    'methoid': 'GET',\n",
        "    'scheme': 'https',\n",
        "    'user-agent': 'Mozilla/5.0'\n",
        "  }\n",
        "\n",
        "  for x in range(1, pages):\n",
        "\n",
        "    page_result = Request(url_based + str(x), headers = params)\n",
        "    page_parser = BeautifulSoup(urlopen(page_result).read(), 'html.parser')\n",
        "    items = page_parser.find('div', {'class': 'sc-1fcmfeb-0'}).find_all('li')\n",
        "\n",
        "    for item in items:\n",
        "\n",
        "      content = item.find('a')\n",
        "\n",
        "      if (content):\n",
        "\n",
        "        try:\n",
        "          \n",
        "          price = content.find('div', {'class': 'fnmrjs-9'}).find('span').get_text()\n",
        "          if (price):\n",
        "            price = price.split('R$')[1]\n",
        "            price = float(price.replace(\".\",\"\"))\n",
        "          else:\n",
        "            price = 0\n",
        "\n",
        "          link = content.get('href')\n",
        "          \n",
        "          data = {\n",
        "            'id': content.get('data-lurker_list_id'),\n",
        "            'link': link,\n",
        "            'text': content.find('h2').get_text(),\n",
        "            'image_home': content.find('img')['src'],\n",
        "            'image_alt': content.find('img')['alt'],\n",
        "            'price': price\n",
        "          }\n",
        "        \n",
        "          if (link):\n",
        "            item_page = Request(link, headers = params)\n",
        "            item_parser = BeautifulSoup(urlopen(item_page).read(), 'html.parser')\n",
        "            item_content = item_parser.find('div', {'class': 'duvuxf-0 h3us20-0 jAHFXn'})\n",
        "\n",
        "            description = item_content.find('span', { 'class': 'sc-1sj3nln-1 eOSweo sc-ifAKCX cmFKIN' }).get_text()\n",
        "\n",
        "            details = {}\n",
        "            items_details = item_content.find('div', {'data-testid': 'ad-properties'}).find_all('div', {'class':'sc-hmzhuo'})\n",
        "            for item in items_details:\n",
        "              elements = item.find_all()\n",
        "              if (elements[0].get_text() == 'Tamanho'):\n",
        "                details[elements[0].get_text()] = elements[1].get_text().split(\"m\")[0]\n",
        "              else:\n",
        "                details[elements[0].get_text()] = elements[1].get_text()\n",
        "              json.dumps(details)\n",
        "            data['details'] = details        \n",
        "            \n",
        "            address = {}\n",
        "            items_locations = item_content.find('div', {'class': 'h3us20-5 keidqa'}).find_all('div', {'class':'sc-hmzhuo sc-1f2ug0x-3 ONRJp sc-jTzLTM iwtnNi'})\n",
        "            for item in items_locations:\n",
        "              elements = item.find_all()\n",
        "              address[elements[0].get_text()] = elements[1].get_text()\n",
        "              json.dumps(address)\n",
        "            data['address'] = address\n",
        "            \n",
        "            json.dumps(data)\n",
        "            results.append(data)\n",
        "            \n",
        "          else:\n",
        "            results.append(data)\n",
        "\n",
        "        except:\n",
        "          continue\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "  return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf2vnh0eLE2N"
      },
      "source": [
        "result_data = getDataOLX(pages=10)\r\n",
        "property.insert(result_data)\r\n",
        "data = pd.json_normalize(result_data)\r\n",
        "data.to_csv('./rent-property-olx.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}